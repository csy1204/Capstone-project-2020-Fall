{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_pickle(x):\n",
    "    with open(x, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_matrix = get_data_from_pickle('tag_matrix.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_numbering = get_data_from_pickle('tag_numbering.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data_version_2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list = [tag for tags in df.tags for tag in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tag_list = list(set(tag_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/joyoungmin/opt/anaconda3/lib/python3.7/site-packages (3.8.3)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/joyoungmin/opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/joyoungmin/opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/joyoungmin/opt/anaconda3/lib/python3.7/site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/joyoungmin/opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: requests in /Users/joyoungmin/opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/joyoungmin/opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/joyoungmin/opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/joyoungmin/opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/joyoungmin/opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "from gensim.models import Word2Vec, FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df.tags.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['까페', '잔잔한']),\n",
       "       list(['운동', '드라이브', 'Pop', '트로피컬하우스', '힐링', '기분전환', '2017', '팝', '트렌드', '일렉']),\n",
       "       list(['잔잔한', '추억', '회상']), ..., list(['인디']), list(['여친']),\n",
       "       list(['설렘', '사랑'])], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m           Word2Vec\n",
       "\u001b[0;31mString form:\u001b[0m    Word2Vec(vocab=1382, size=10, alpha=0.025)\n",
       "\u001b[0;31mFile:\u001b[0m           ~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/word2vec.py\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
       "\n",
       "Once you're finished training a model (=no more updates, only querying)\n",
       "store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `self.wv` to reduce memory.\n",
       "\n",
       "The model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
       ":meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
       "\n",
       "The trained word vectors can also be stored/loaded from a format compatible with the\n",
       "original word2vec implementation via `self.wv.save_word2vec_format`\n",
       "and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
       "\n",
       "Some important attributes are the following:\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
       "    This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
       "    directly to query those embeddings in various ways. See the module level docstring for examples.\n",
       "\n",
       "vocabulary : :class:`~gensim.models.word2vec.Word2VecVocab`\n",
       "    This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n",
       "    Besides keeping track of all unique words, this object provides extra functionality, such as\n",
       "    constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n",
       "\n",
       "trainables : :class:`~gensim.models.word2vec.Word2VecTrainables`\n",
       "    This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n",
       "    network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n",
       "    a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n",
       "    (which means that the size of the hidden layer is equal to the number of features `self.size`).\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Parameters\n",
       "----------\n",
       "sentences : iterable of iterables, optional\n",
       "    The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
       "    consider an iterable that streams the sentences directly from disk/network.\n",
       "    See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
       "    or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
       "    See also the `tutorial on data streaming in Python\n",
       "    <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
       "    If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
       "    in some other way.\n",
       "corpus_file : str, optional\n",
       "    Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
       "    You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
       "    `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
       "size : int, optional\n",
       "    Dimensionality of the word vectors.\n",
       "window : int, optional\n",
       "    Maximum distance between the current and predicted word within a sentence.\n",
       "min_count : int, optional\n",
       "    Ignores all words with total frequency lower than this.\n",
       "workers : int, optional\n",
       "    Use these many worker threads to train the model (=faster training with multicore machines).\n",
       "sg : {0, 1}, optional\n",
       "    Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
       "hs : {0, 1}, optional\n",
       "    If 1, hierarchical softmax will be used for model training.\n",
       "    If 0, and `negative` is non-zero, negative sampling will be used.\n",
       "negative : int, optional\n",
       "    If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
       "    should be drawn (usually between 5-20).\n",
       "    If set to 0, no negative sampling is used.\n",
       "ns_exponent : float, optional\n",
       "    The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
       "    to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
       "    than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
       "    More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
       "    other values may perform better for recommendation applications.\n",
       "cbow_mean : {0, 1}, optional\n",
       "    If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
       "alpha : float, optional\n",
       "    The initial learning rate.\n",
       "min_alpha : float, optional\n",
       "    Learning rate will linearly drop to `min_alpha` as training progresses.\n",
       "seed : int, optional\n",
       "    Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
       "    the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
       "    you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
       "    from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
       "    use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
       "max_vocab_size : int, optional\n",
       "    Limits the RAM during vocabulary building; if there are more unique\n",
       "    words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
       "    Set to `None` for no limit.\n",
       "max_final_vocab : int, optional\n",
       "    Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
       "    min_count is more than the calculated min_count, the specified min_count will be used.\n",
       "    Set to `None` if not required.\n",
       "sample : float, optional\n",
       "    The threshold for configuring which higher-frequency words are randomly downsampled,\n",
       "    useful range is (0, 1e-5).\n",
       "hashfxn : function, optional\n",
       "    Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
       "iter : int, optional\n",
       "    Number of iterations (epochs) over the corpus.\n",
       "trim_rule : function, optional\n",
       "    Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
       "    be trimmed away, or handled using the default (discard if word count < min_count).\n",
       "    Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
       "    or a callable that accepts parameters (word, count, min_count) and returns either\n",
       "    :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
       "    The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
       "    model.\n",
       "\n",
       "    The input parameters are of the following types:\n",
       "        * `word` (str) - the word we are examining\n",
       "        * `count` (int) - the word's frequency count in the corpus\n",
       "        * `min_count` (int) - the minimum count threshold.\n",
       "sorted_vocab : {0, 1}, optional\n",
       "    If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
       "    See :meth:`~gensim.models.word2vec.Word2VecVocab.sort_vocab()`.\n",
       "batch_words : int, optional\n",
       "    Target size (in words) for batches of examples passed to worker threads (and\n",
       "    thus cython routines).(Larger batches will be passed if individual\n",
       "    texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
       "compute_loss: bool, optional\n",
       "    If True, computes and stores loss value which can be retrieved using\n",
       "    :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
       "callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
       "    Sequence of callbacks to be executed at specific stages during training.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
       "\n",
       ".. sourcecode:: pycon\n",
       "\n",
       "    >>> from gensim.models import Word2Vec\n",
       "    >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
       "    >>> model = Word2Vec(sentences, min_count=1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CBOW(sg=0) : 주어진 단어에 앞뒤로 몇개의 단어를 Input으로 사용하여 주어진 단어를 유추하는 모델\n",
    "# skip-gram(sg=1) : 현재 단어 하나를 가지고 주위에 등장하는 몇개의 단어들의 등장 유무를 유추\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.05 s, sys: 19.2 ms, total: 2.07 s\n",
      "Wall time: 798 ms\n"
     ]
    }
   ],
   "source": [
    "#임베딩(w2v_skipgram)모델 생성\n",
    "%time Skip_Gram_model = Word2Vec(corpus, size=10, window=10, min_count=1,  workers=8, sg=1, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyoungmin/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('휴양지', 0.8958439826965332),\n",
       " ('휴일', 0.8623803853988647),\n",
       " ('해변', 0.8599976301193237),\n",
       " ('기분전환', 0.8586515188217163),\n",
       " ('드라이브', 0.8563899993896484),\n",
       " ('운전', 0.8547537326812744),\n",
       " ('비행기', 0.8532578945159912),\n",
       " ('바캉스', 0.8526214361190796),\n",
       " ('여름', 0.8491854667663574),\n",
       " ('바다', 0.8478224277496338)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Skip_Gram_model.most_similar('여행', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyoungmin/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('크리스마스캐롤', 0.9929973483085632),\n",
       " ('메리크리스마스', 0.9926170110702515),\n",
       " ('크리스마스노래', 0.9914136528968811),\n",
       " ('Christmas', 0.988018274307251),\n",
       " ('캐럴', 0.9838935136795044),\n",
       " ('크리스마스이브', 0.9802675247192383),\n",
       " ('캐롤', 0.9665442109107971),\n",
       " ('크리스마스', 0.945396900177002),\n",
       " ('12월', 0.9253143072128296),\n",
       " ('겨울노래', 0.9234572649002075)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Skip_Gram_model.most_similar('성탄절', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyoungmin/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "tag_vectors = []\n",
    "for tag in unique_tag_list:\n",
    "    tag_vectors.append(Skip_Gram_model[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.19348139,  0.56510955, -0.43994173,  0.41321546, -0.8927941 ,\n",
       "       -0.25548512, -0.14549626,  0.2310634 , -0.2999666 ,  0.793347  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_vectors[1381]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 인퍼런스 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans2 = KMeans(n_clusters=2, random_state=0).fit(tag_vectors)\n",
    "# kmeans4 = KMeans(n_clusters=4, random_state=0).fit(tag_vectors)\n",
    "# kmeans6 = KMeans(n_clusters=6, random_state=0).fit(tag_vectors)\n",
    "# kmeans8 = KMeans(n_clusters=8, random_state=0).fit(tag_vectors)\n",
    "kmeans10 = KMeans(n_clusters=10, random_state=0).fit(tag_vectors)\n",
    "# kmeans12 = KMeans(n_clusters=12, random_state=0).fit(tag_vectors)\n",
    "# kmeans14 = KMeans(n_clusters=14, random_state=0).fit(tag_vectors)\n",
    "# kmeans16 = KMeans(n_clusters=16, random_state=0).fit(tag_vectors)\n",
    "# kmeans18 = KMeans(n_clusters=18, random_state=0).fit(tag_vectors)\n",
    "# kmeans20 = KMeans(n_clusters=20, random_state=0).fit(tag_vectors)\n",
    "kmeans15 = KMeans(n_clusters=15, random_state=0).fit(tag_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 65 \n",
      " ['결혼식', '따스함', '고막남친', '여친', '연애', '커플', '연인', '달달한', '고백송', '달달한노래', '봄바람', '설렘']\n",
      "4 286 \n",
      " ['Queen', '칠아웃', '위클리초이스', '편곡', '졸릴때', 'J_Rock', '팝뮤직', '남녀듀엣', '남성보컬', '시부야케이', '전통가요', '전곡듣기']\n",
      "10 70 \n",
      " ['electronica', '내적댄스', '디스코', '딥하우스', '러닝머신', 'Club', '이디엠', '일렉트로니카', '웨이트', '사운드', '흥폭발', '파티']\n",
      "3 173 \n",
      " ['간지', '한국힙합', '팝송', '댄스음악', '1980', '광고', '시상식', '덥스텝', '웹진웨이브', '파워워킹', '멋진', '딘']\n",
      "0 76 \n",
      " ['지친하루', '노동요', '직장인', '힐링음악', '그냥', '위로가필요할때', '월요병', '청춘', '부드러운_잔잔한', '버스', '지친', '설날']\n",
      "13 101 \n",
      " ['방콕', '더위', '시원한', '기차', '경쾌', '자전거', '비행기', '봄나들이', '공원', '귀여운', '한강', '여름']\n",
      "1 99 \n",
      " ['싱그러운', '선물', '팝재즈', '잔잔한음악', '재즈', '조용', '카페음악', '강아지', '배경음악', '커피', '어쿠스틱팝', '쌀쌀']\n",
      "14 101 \n",
      " ['음색깡패', '라운지', '스윗마인드', 'drive', '알앤비소울', '빈티지', '리듬감', '끈적', '매장음악', '장르불문', 'Trendy', '그루브']\n",
      "9 23 \n",
      " ['메리크리스마스', '눈오는날', '겨울밤', '쌀쌀한', '크리스마스', '크리스마스노래', '겨울노래', '캐롤', '추위', '12월', '겨울감성', '추운']\n",
      "12 41 \n",
      " ['사색', '잠', '연주곡', '치유', '낮잠', '뉴에이지', '편안한', '책읽을때', '공부할때', '아기', '어린이집', '자장가']\n",
      "2 63 \n",
      " ['레드벨벳', '성인가요', '회식', '영탁', '남자아이돌', '내마음의사진', '아이돌숨은명곡', '2019년', '아이돌그룹', '방탄소년단', '에이핑크', '아이돌의숨은명곡']\n",
      "5 93 \n",
      " ['락', '한국', 'Soundtrack', '가요', '사랑설렘', '가사로듣는', '감동', '장르', '숨은명곡', '드라마ost', '애니메이션', '여자가수']\n",
      "8 111 \n",
      " ['감성적인', '우울', '고요한', '잠자기전', '새벽', '감성인디', '감성곡', '목소리', '비올때', '우울한날', '마무리', '선선한']\n",
      "11 41 \n",
      " ['HipHop', '공연', 'HIPHOPLE', '호주', 'SWAG', '힙합엘이', '외국힙합', 'UK', '국내힙합', '랩', 'M에센셜', '흑인음악']\n",
      "7 39 \n",
      " ['홀로', '헤어짐', '이별노래', '권태기', '눈물', '가사', '솔로', '기억', '아련', '상처', '보고싶다', '슬픔']\n"
     ]
    }
   ],
   "source": [
    "tag_dict = defaultdict(list)\n",
    "for tag, label in zip(unique_tag_list, kmeans15.labels_):\n",
    "    tag_dict[label].append(tag)\n",
    "for idx in tag_dict.keys():\n",
    "    print(idx, len(tag_dict[idx]),\"\\n\", tag_dict[idx][:12])    \n",
    "# for i in range(6):\n",
    "#     print(i, tag_dict[i][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 76 \n",
      " ['지친하루', '노동요', '직장인', '힐링음악', '그냥', '위로가필요할때', '월요병', '청춘', '부드러운_잔잔한', '버스', '지친', '설날']\n",
      "1 99 \n",
      " ['싱그러운', '선물', '팝재즈', '잔잔한음악', '재즈', '조용', '카페음악', '강아지', '배경음악', '커피', '어쿠스틱팝', '쌀쌀']\n",
      "2 63 \n",
      " ['레드벨벳', '성인가요', '회식', '영탁', '남자아이돌', '내마음의사진', '아이돌숨은명곡', '2019년', '아이돌그룹', '방탄소년단', '에이핑크', '아이돌의숨은명곡']\n",
      "3 173 \n",
      " ['간지', '한국힙합', '팝송', '댄스음악', '1980', '광고', '시상식', '덥스텝', '웹진웨이브', '파워워킹', '멋진', '딘']\n",
      "4 286 \n",
      " ['Queen', '칠아웃', '위클리초이스', '편곡', '졸릴때', 'J_Rock', '팝뮤직', '남녀듀엣', '남성보컬', '시부야케이', '전통가요', '전곡듣기']\n",
      "5 93 \n",
      " ['락', '한국', 'Soundtrack', '가요', '사랑설렘', '가사로듣는', '감동', '장르', '숨은명곡', '드라마ost', '애니메이션', '여자가수']\n",
      "6 65 \n",
      " ['결혼식', '따스함', '고막남친', '여친', '연애', '커플', '연인', '달달한', '고백송', '달달한노래', '봄바람', '설렘']\n",
      "7 39 \n",
      " ['홀로', '헤어짐', '이별노래', '권태기', '눈물', '가사', '솔로', '기억', '아련', '상처', '보고싶다', '슬픔']\n",
      "8 111 \n",
      " ['감성적인', '우울', '고요한', '잠자기전', '새벽', '감성인디', '감성곡', '목소리', '비올때', '우울한날', '마무리', '선선한']\n",
      "9 23 \n",
      " ['메리크리스마스', '눈오는날', '겨울밤', '쌀쌀한', '크리스마스', '크리스마스노래', '겨울노래', '캐롤', '추위', '12월', '겨울감성', '추운']\n",
      "10 70 \n",
      " ['electronica', '내적댄스', '디스코', '딥하우스', '러닝머신', 'Club', '이디엠', '일렉트로니카', '웨이트', '사운드', '흥폭발', '파티']\n",
      "11 41 \n",
      " ['HipHop', '공연', 'HIPHOPLE', '호주', 'SWAG', '힙합엘이', '외국힙합', 'UK', '국내힙합', '랩', 'M에센셜', '흑인음악']\n",
      "12 41 \n",
      " ['사색', '잠', '연주곡', '치유', '낮잠', '뉴에이지', '편안한', '책읽을때', '공부할때', '아기', '어린이집', '자장가']\n",
      "13 101 \n",
      " ['방콕', '더위', '시원한', '기차', '경쾌', '자전거', '비행기', '봄나들이', '공원', '귀여운', '한강', '여름']\n",
      "14 101 \n",
      " ['음색깡패', '라운지', '스윗마인드', 'drive', '알앤비소울', '빈티지', '리듬감', '끈적', '매장음악', '장르불문', 'Trendy', '그루브']\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print(i, len(tag_dict[i]),\"\\n\",tag_dict[i][:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict_numerical = tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    for j in range(len(tag_dict_numerical[i])):\n",
    "        tag_dict_numerical[i][j] = tag_numbering[tag_dict_numerical[i][j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
