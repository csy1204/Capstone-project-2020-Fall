# -*- coding: utf-8 -*-
"""Cluster.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uLXe4JtvFfWVGqdx7kaaVKK4FaNhJzYW
"""

import os
import glob
import pandas as pd
import numpy as np
import json

from tqdm.notebook import tqdm

from collections import defaultdict
import random

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model

import pickle

def get_path(filename):
  return f'/content/drive/MyDrive/Data/{filename}'

def get_data_from_pickle(path):
    with open(path, "rb") as f:
        data = pickle.load(f)
    return data

def write_data_to_pickle(data, path):
  with open(path, "wb") as f:
      f.write(pickle.dumps(data))

tag_vectors = get_data_from_pickle(get_path('tag_vectors'))
cluster_centers = get_data_from_pickle(get_path('kmeans15.cluster_centers_.pickle'))
unique_tag_list = get_data_from_pickle(get_path('unique_tag_list.pickle'))
tag_dict = get_data_from_pickle(get_path('tag_dict.pickle'))

tag_matrix = np.array(tag_vectors)

df_meta = pd.read_csv(get_path('Meta_v1.csv'))
# df = pd.read_json(get_path('data_version_2.json'))

df_train = pd.read_csv(get_path('train_v7.csv'))
df_test = pd.read_csv(get_path('test_v7.csv'))

tag_dict[8]

tag2idx = {tag: i for i, tag in enumerate(unique_tag_list)}
idx2tag = {i: tag for i, tag in enumerate(unique_tag_list)}

# df = df.reset_index(drop=True)

# unique_songs = list(set([song for songs in df.songs2 for song in songs]))

# song_idx2new_idx = {song_id: idx for idx, song_id in enumerate(unique_songs)}
# new_idx2song_idx = {idx: song_id for idx, song_id in enumerate(unique_songs)}

# genres_set = list(set([genre for genres in df_meta.gnrs for genre in genres]))

# df_meta['new_id'] = df_meta.id.map(lambda x: song_idx2new_idx.get(int(x)))

df_train

df_train[df_train.tag_id.map(lambda x: idx2tag.get(x)) == "성탄절"]

df_meta[df_meta.id == 338900]



# df['tagIds'] = df.tags.map(lambda x: [tag2idx.get(tag) for tag in x])

tag_cluster_ids = [
    [tag2idx[tag] for tag in tag_dict[key]]
    for key in range(15)
]

# min_tag_count = 2

# df_clusters = [
#     df[df.tagIds.map(lambda x: np.intersect1d(tag_cluster_ids[key], x, assume_unique=True).shape[0] >= min_tag_count )]
#     for key in range(15)
# ]

# cluster_songs = []

# for key in range(15):
#     cluster_songs.append(list(set([song for songs in df_clusters[key].songs2 for song in songs])))
#     print(f"{key} : {df_clusters[key].shape} / {len(cluster_songs[key])}")

def get_recommendation_model(tag_num, music_num, embedding_matrix, latent_features=10, trainable=True):

  tag = Input(shape=(1,), dtype='int32')
  item = Input(shape=(1,), dtype='int32')

  # User embedding for GMF

  gmf_tag_embedding = Embedding(
            embedding_matrix.shape[0],
            embedding_matrix.shape[1],
            input_length=1,
            embeddings_initializer=keras.initializers.Constant(embedding_matrix),
            trainable=trainable,
            name='tag_embedding'
        )(tag)
  
  # gmf_tag_embedding = Embedding(tag_num, latent_features, input_length=user.shape[1])(tag)
  gmf_tag_embedding = Flatten()(gmf_tag_embedding)

  # Item embedding for GMF
  gmf_item_embedding = Embedding(music_num, latent_features, input_length=item.shape[1])(item)
  gmf_item_embedding = Flatten()(gmf_item_embedding)

  # GMF layers
  gmf_mul =  Multiply()([gmf_tag_embedding, gmf_item_embedding])


  # User embedding for MLP
  mlp_tag_embedding = Embedding(
            embedding_matrix.shape[0],
            embedding_matrix.shape[1],
            input_length=1,
            embeddings_initializer=keras.initializers.Constant(embedding_matrix),
            trainable=trainable,
            name='tag_embedding2'
        )(tag)
  
  mlp_tag_embedding = Flatten()(mlp_tag_embedding)

  # Item embedding for MLP
  mlp_item_embedding = Embedding(music_num, 32, input_length=item.shape[1])(item)
  mlp_item_embedding = Flatten()(mlp_item_embedding)

  # MLP layers
  mlp_concat = Concatenate()([mlp_tag_embedding, mlp_item_embedding])
  mlp_dropout = Dropout(0.2)(mlp_concat)

  # Layer1
  mlp_layer_1 = Dense(units=64, activation='relu', name='mlp_layer1')(mlp_dropout)  # (64,1)
  mlp_dropout1 = Dropout(rate=0.2, name='dropout1')(mlp_layer_1)                    # (64,1)
  mlp_batch_norm1 = BatchNormalization(name='batch_norm1')(mlp_dropout1)            # (64,1)

  # Layer2
  mlp_layer_2 = Dense(units=32, activation='relu', name='mlp_layer2')(mlp_batch_norm1)  # (32,1)
  mlp_dropout2 = Dropout(rate=0.2, name='dropout2')(mlp_layer_2)                        # (32,1)
  mlp_batch_norm2 = BatchNormalization(name='batch_norm2')(mlp_dropout2)                # (32,1)

  # Layer3
  mlp_layer_3 = Dense(units=16, activation='relu', name='mlp_layer3')(mlp_batch_norm2)  # (16,1)

  # Layer4
  mlp_layer_4 = Dense(units=8, activation='relu', name='mlp_layer4')(mlp_layer_3)       # (8,1)



  # merge GMF + MLP
  merged_vector = tf.keras.layers.concatenate([gmf_mul, mlp_layer_4]) # (16)

  # Output layer
  output_layer = Dense(1, kernel_initializer='lecun_uniform', name='output_layer')(merged_vector) # 1,1 / h(8,1)초기화

  # Model
  model = Model([tag, item], output_layer)
  model.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics=["mse"])
  return model

class AllTagClusterModel(object):
    
    def __init__(self, _df, tag2idx):
        self._df = _df
        
        self.tag2id = tag2idx
        self._max_tag_len = len(tag2idx.keys())
        self._unique_tag_list = list(tag2idx.keys())
        
        self._unique_songs = list(set([song for songs in _df.songs2 for song in songs]))
        self._song_idx2new_idx = {song_id: idx for idx, song_id in enumerate(self._unique_songs)}
        self._new_idx2song_idx = {idx: song_id for idx, song_id in enumerate(self._unique_songs)}
        self._max_song_len = len(self._unique_songs)
        
        self._df['_song_ids'] = _df.songs2.map(lambda x: [self._song_idx2new_idx.get(song) for song in x])
        
        
    def preprocess(self, test_size=0.2, NEG_SAMPLE_NUM=20):
        _tag_song_freq = {i: defaultdict(int) for i in range(self._max_tag_len)}
        
        for tags, songs in tqdm(zip(self._df.tagIds, self._df._song_ids)):
            for song in songs:
                for tag in tags:
                    _tag_song_freq[tag][song] += 1
                    
        _df_freq = pd.DataFrame(_tag_song_freq).T
        _df_freq = _df_freq.stack().reset_index()
        _df_freq.columns = ['tag_id', 'song_id', 'freq']
        
        neg_data = []
        
        for tag in tqdm(self._unique_tag_list):
            i = NEG_SAMPLE_NUM
            new_tag_id = self.tag2id[tag]
            while i:
                random_id = self.get_random_music_id(self._max_song_len)
                if not _tag_song_freq[new_tag_id][random_id]:
                    neg_data.append((new_tag_id, random_id, 0))
                    i -= 1
        
        _df_neg = pd.DataFrame(neg_data, columns=_df_freq.columns)
        _df_freq = pd.concat([_df_freq, _df_neg]).sample(frac=1).reset_index(drop=True)
        _df_freq['freq2'] = _df_freq.freq.map(lambda x: 0.5 if x == 1 else (0 if x == 0 else 1))

        _df_over = _df_freq[_df_freq.freq > 1]
        
        test = _df_over.groupby('tag_id').apply(lambda x: self.get_test_sample(x, test_size)).reset_index()
        self.test = test.sample(frac=1).reset_index(drop=True)
        
        self.train = _df_freq[~_df_freq.index.isin(self.test.level_1)]
        
        self.x_train = [ 
                        self.train.tag_id.values.reshape(-1,1),
                        self.train.song_id.values.reshape(-1,1)
                        ]
        self.y_train = self.train.freq2.values
        
        self.x_test = [ 
                       self.test.tag_id.values.reshape(-1,1),
                       self.test.song_id.values.reshape(-1,1)
                       ]
        self.y_test = self.test.freq2.values
        
        
        
    def build_model(self, tag_matrix, EMBEDDING_SIZE=10, trainable=True):
        self.model = get_recommendation_model(self._max_tag_len, self._max_song_len, tag_matrix, EMBEDDING_SIZE, trainable)
        
    def fit(self, batch_size=64, epochs=10):
        self.history = self.model.fit(
            x=self.x_train,
            y=self.y_train,
            batch_size=batch_size,
            epochs=epochs,
            verbose=1,
            validation_data=(self.x_test, self.y_test),
        )
        
    def print_summary(self):
      print(self.model.summary())
        
    def predict(self, tag, k=50):
      _test_tag_id = self.tag2id.get(tag)

      if _test_tag_id == None: return []

      tags_input = np.array([_test_tag_id] * self._max_song_len).reshape(-1, 1)
      songs_input = np.array([*range(self._max_song_len)]).reshape(-1, 1)
      
      predicted = self.model.predict([tags_input, songs_input]).flatten()
      predicted = predicted.argsort()[-k:][::-1]

      return [self._new_idx2song_idx.get(new_id) for new_id in predicted]
        
    @staticmethod
    def get_test_sample(df, test_size=0.2, drop_column='tag_id'):
        return df.sample(frac=test_size).drop(drop_column, axis=1)
        
    @staticmethod
    def get_random_music_id(_max_len):
        return random.randint(0, _max_len-1)

# train['song_id2'] = train.song_id.map(lambda x: all_tag_model._new_idx2song_idx[x])
# test['song_id2'] = test.song_id.map(lambda x: all_tag_model._new_idx2song_idx[x])

train_clusters = [df_train[df_train.tag_id.isin(tag_cluster_ids[key])] for key in range(15)]

for i, train_df in enumerate(train_clusters):
  print(f"{i} {train_df.shape} / song: {train_df.song_id.nunique()} tag: {train_df.tag_id.nunique()}")

train_freq_clusters = []
for i in range(15):
  song_freq_cluster = train_clusters[i].song_id.value_counts()
  freq_song_ids = song_freq_cluster[song_freq_cluster > 3].index.values
  train_freq_clusters.append(train_clusters[i][train_clusters[i].song_id.isin(freq_song_ids)])
  print(f"{i}: {train_freq_clusters[i].shape} / {freq_song_ids.shape}")

df_train

class ClusterModel(object):
    
    def __init__(self, train, tag2idx):
        self.song_freq_cluster = train.song_id.value_counts()
        self.freq_song_ids  = self.song_freq_cluster[self.song_freq_cluster > 2].index.values
        self.train = train[train.song_id.isin(self.freq_song_ids)]

        self.tag2id = tag2idx
        self._max_tag_len = len(tag2idx.keys())
        self._unique_tag_list = list(tag2idx.keys())
        
        self._unique_songs = self.freq_song_ids
        self._max_song_len = len(self._unique_songs)

        self._song_idx2new_idx = {song_id: idx for idx, song_id in enumerate(self._unique_songs)}
        self._new_idx2song_idx = {idx: song_id for idx, song_id in enumerate(self._unique_songs)}

        self.train['_song_id'] = self.train.song_id.map(lambda song_id: self._song_idx2new_idx.get(song_id))

        self.x_train = [ 
                        self.train.tag_id.values.reshape(-1,1),
                        self.train._song_id.values.reshape(-1,1)
                        ]
        
        self.y_train = self.train.freq2.values

    def build_model(self, tag_matrix, EMBEDDING_SIZE=10, trainable=True):
        self.model = get_recommendation_model(self._max_tag_len, self._max_song_len, tag_matrix, EMBEDDING_SIZE, trainable)
        
    def fit(self, batch_size=64, epochs=10):
        self.history = self.model.fit(
            x=self.x_train,
            y=self.y_train,
            batch_size=batch_size,
            epochs=epochs,
            verbose=1
        )
        
    def print_summary(self):
      print(self.model.summary())
        
    def predict(self, tag, k=50):
      _test_tag_id = self.tag2id.get(tag)

      if _test_tag_id == None: return []

      tags_input = np.array([_test_tag_id] * self._max_song_len).reshape(-1, 1)
      songs_input = np.array([*range(self._max_song_len)]).reshape(-1, 1)
      
      predicted = self.model.predict([tags_input, songs_input]).flatten()
      predicted = predicted.argsort()[-k:][::-1]

      return [self._new_idx2song_idx.get(new_id) for new_id in predicted]

Clusters = [
    ClusterModel(train_freq_clusters[i], tag2idx)
    for i in range(15)
]

for i in range(15):
  Clusters[i].build_model(tag_matrix)

for i in range(15):
  print(f"Model {i}")
  Clusters[i].fit()

test_clusters = [df_test[df_test.tag_id.isin(tag_cluster_ids[key])] for key in range(15)]

df_test[df_test.tag_id.isin(tag_cluster_ids[0])]

test_clusters[0]

test_clusters[0].song_id.unique().shape

Clusters[0]._unique_songs.shape

total_songs = np.intersect1d(test_clusters[8].song_id.unique(), Clusters[8]._unique_songs)

total_songs.shape

# np.intersect1d(train_clusters[8].song_id, test_clusters[8].song_id).shape



# k = 50

# x_test =  all_tag_model.x_test
# test_tags = list(set(x_test[0].ravel()))

# recall_result = []
# precision_result = []
# prediction_result = []
# true_result = []
# correct_result = []

# spent_result = []

# for _test_tag_id in tqdm(test_tags):
#     song_ids = [
#         all_tag_model._new_idx2song_idx.get(new_id)
#         for new_id in x_test[1][x_test[0] == _test_tag_id]
#     ]
#     start_time = time.time()
#     predicted = all_tag_model.predict(idx2tag[_test_tag_id], k)
#     spent_result.append(time.time() - start_time)

#     correct_count = np.intersect1d(song_ids, predicted).shape[0]

#     correct_result.append(correct_count)
#     prediction_result.append(predicted)
#     true_result.append(song_ids)
#     recall_result.append(correct_count / len(song_ids))
#     precision_result.append(correct_count / k)



x_test.song_id.unique().shape

import time



tag_names = [idx2tag[tag_id] for tag_id in test_tags]
df_result = pd.DataFrame([tag_names, recall_result, precision_result]).T

df_result[1].mean()

df_result[2].mean()

df_train.pivot_table('freq', 'tag_id', 'song_id').fillna(0)

df_result = pd.read_json(get_path('Result_v2.json'))

good_tags = df_result[df_result.precision > 0].tag.map(lambda x: tag2idx.get(x)).values

df_test

df_test2 = df_test[df_test.tag_id.isin(good_tags)]

df_test2 = df_test2.reset_index(drop=True)

test_clusters = [df_test2[df_test2.tag_id.isin(tag_cluster_ids[key])] for key in range(15)]

import time

for i in range(df_test2.tag_id)



good_tags_id = [tag2idx.get(tag) for tag in good_tags]

df_test2 = df_test[df_test.tag_id.isin(good_tags_id)].reset_index(drop=True)

recall_result = []
precision_result = []
prediction_result = []

true_result = []
correct_result = []
spent_result = []

# x_test =  test_clusters[idx]
test_tags = df_test2['tag_id'].unique()
total_songs = np.intersect1d(df_test2.song_id.unique(), df_train.song_id.unique())

k = 50

for _test_tag_id in tqdm(test_tags):
    song_ids = np.intersect1d(x_test.song_id[x_test.tag_id == _test_tag_id].values, total_songs)

    start_time = time.time()
    
    predicted = []
    for key_id, weight in clusters_and_weight(_test_tag_id).items():
      predicted.extend(Clusters[key_id].predict(idx2tag[_test_tag_id])[:int(k*weight)])
    
    spent_result.append(time.time() - start_time)
    correct_count = np.intersect1d(song_ids, predicted).shape[0]

    correct_result.append(correct_count)
    prediction_result.append(predicted)
    true_result.append(song_ids)

    if len(song_ids) == 0:
      recall_result.append(0)
      precision_result.append(0)
      continue
    
    recall_result.append(correct_count / len(song_ids))
    precision_result.append(correct_count / k)

tag_names = [idx2tag[tag_id] for tag_id in test_tags]
df_result = pd.DataFrame([tag_names, recall_result, precision_result, prediction_result, true_result, correct_result, spent_result]).T
df_result.columns = ['tag', 'recall', 'precision', 'prediction', 'true', 'correct', 'spent']

len([
 clusters_and_weight(_test_tag_id)
 for _test_tag_id in tqdm(test_tags)
 if len(clusters_and_weight(_test_tag_id).items()) > 1
])

test_tags

prediction_result = []



k = 10
for _test_tag_id in tqdm(test_tags):
    song_ids = np.intersect1d(x_test.song_id[x_test.tag_id == _test_tag_id].values, total_songs)
    predicted = []
    for key_id, weight in clusters_and_weight(_test_tag_id).items():
      predicted.extend(Clusters[key_id].predict(idx2tag[_test_tag_id], k=int(k*weight)))
    prediction_result.append(predicted)

df_result = df_result.merge(pd.DataFrame({'tag': tag_names, 'k10_prediction': prediction_result}), on='tag')

df_result['k10'] = [np.intersect1d(pred, true).shape[0] for pred, true in zip(df_result.k10_prediction, df_result.true)]
df_result['recall@10'] = df_result.k10 / df_result.true.map(lambda x: len(x))
df_result['precision@10'] = df_result.k10 / 10



df_result.k10.sum(), df_result_all2.k10.sum()

df_result.k25.sum(), df_result_all2.k25.sum()

df_result.mean()

df_result_all2.mean()

df_result.k25.mean()

# for idx, (v2_true, v1_true) in enumerate(zip(df_result.true, df_result_all2.true)):
#   if set(v1_true) != set(v2_true):
#     print(idx, len(v1_true), len(v2_true))

x_test.song_id[x_test.tag_id == _test_tag_id]

df_result

df_result.true = df_result.tag.map(lambda x: np.intersect1d(df_test.song_id[df_test.tag_id == tag2idx.get(x)].values, total_songs))

df_result['k50'] = [np.intersect1d(pred, true).shape[0] for pred, true in zip(df_result.prediction, df_result.true)]
df_result['k25'] = [np.intersect1d(pred[:25], true).shape[0] for pred, true in zip(df_result.prediction, df_result.true)]
df_result['k10'] = [np.intersect1d(pred[:10], true).shape[0] for pred, true in zip(df_result.prediction, df_result.true)]

df_result['recall@50'] = df_result.k50 / df_result.true.map(lambda x: len(x))
df_result['recall@25'] = df_result.k25 / df_result.true.map(lambda x: len(x))
df_result['recall@10'] = df_result.k10 / df_result.true.map( lambda x: len(x))

df_result['precision@50'] = df_result.k50 / 50
df_result['precision@25'] = df_result.k25 / 25
df_result['precision@10'] = df_result.k10 / 10

# df_result.k50.sum(), df_result_all2.k50.sum()

df_result.k50.sum() / (df_result.shape[0] * 50)

df_result.k25.sum() / (df_result.shape[0] * 25)

df_result.k10.sum() / (df_result.shape[0] * 10)

df_result_all2.k10.sum() / (df_result_all2.shape[0] * 10)

df_result_all2['true2'] = df_result.true

df_result_all2

df_result_all2['k50'] = [np.intersect1d(pred, true).shape[0] for pred, true in zip(df_result_all2.prediction, df_result_all2.true)]
df_result_all2['k25'] = [np.intersect1d(pred[:25], true).shape[0] for pred, true in zip(df_result_all2.prediction, df_result_all2.true)]
df_result_all2['k10'] = [np.intersect1d(pred[:10], true).shape[0] for pred, true in zip(df_result_all2.prediction, df_result_all2.true)]

df_result_all2['recall@50'] = df_result_all2.k50 / df_result_all2.true.map(lambda x: len(x))
df_result_all2['recall@25'] = df_result_all2.k25 / df_result_all2.true.map(lambda x: len(x))
df_result_all2['recall@10'] = df_result_all2.k10 / df_result_all2.true.map(lambda x: len(x))

df_result_all2['k50'] = [np.intersect1d(pred, true).shape[0] for pred, true in zip(df_result_all2.prediction, df_result_all2.true2)]
df_result_all2['k25'] = [np.intersect1d(pred[:25], true).shape[0] for pred, true in zip(df_result_all2.prediction, df_result_all2.true2)]
df_result_all2['k10'] = [np.intersect1d(pred[:10], true).shape[0] for pred, true in zip(df_result_all2.prediction, df_result_all2.true2)]

df_result_all2['recall@50'] = df_result_all2.k50 / df_result_all2.true2.map(lambda x: len(x))
df_result_all2['recall@25'] = df_result_all2.k25 / df_result_all2.true2.map(lambda x: len(x))
df_result_all2['recall@10'] = df_result_all2.k10 / df_result_all2.true2.map(lambda x: len(x))





df_result.mean() - df_result_all2.mean()

df_result_all2['precision@50'] = df_result_all2.k50 / 50
df_result_all2['precision@25'] = df_result_all2.k25 / 25
df_result_all2['precision@10'] = df_result_all2.k10 / 10

df_result.k50 == df_result_all2.k50

for i in range(15):
  Clusters[i].model.save(get_path(f'cluster_model_{i}'))
  write_data_to_pickle(Clusters[i]._new_idx2song_idx, get_path(f'cluster_model_{i}_new2idx2songidx.pickle'))

df_result_all2.to_json(get_path('cluster_v1_final_result.json'))
df_result.to_json(get_path('cluster_v2_final_result.json'))

new_model = tf.keras.models.load_model(get_path('cluster_model_8'))
new_new_idx2song_idx = get_data_from_pickle(get_path('cluster_model_8_new2idx2songidx.pickle'))

# new_new_idx2song_idx

new_model.predict()

_test_tag_id = tag2idx.get('눈오는날')

_test_tag_id

_max_song_len = len(new_new_idx2song_idx)
tags_input = np.array([_test_tag_id] * _max_song_len).reshape(-1, 1)
songs_input = np.array([*range(_max_song_len)]).reshape(-1, 1)
predicted = new_model.predict([tags_input, songs_input]).flatten()

predicted = predicted.argsort()[-50:][::-1]

df_meta[df_meta.id.isin([new_new_idx2song_idx.get(new_id) for new_id in predicted])]

df_result = df_result.sort_values('tag').reset_index(drop=True)

df_result_all2 = df_result_all2.sort_values('tag').reset_index(drop=True)

df_result_all2

not_same_count = 0
for v1_pred, v2_pred in zip(df_result_all2.prediction, df_result.prediction):
  if set(v1_pred) != set(v2_pred):
    not_same_count += 1
print(not_same_count)



df_result.mean()

df_result_all2.mean()

df_result_all2

df_result_all2.groupby('cluster')['recall@50','recall@25','recall@10'].mean().plot.bar(figsize=(16,9))

df_result_all2.groupby('cluster')['precision@50','precision@25','precision@10'].mean().plot.bar(figsize=(16,9))



df_result_all2.groupby('cluster')['spent'].mean().plot.bar(figsize=(16,9))

df_result[['recall@50','recall@25','recall@10']].mean().plot.bar(figsize=(16,9))

df_result[['precision@50','precision@25','precision@10']].mean().plot.bar(figsize=(16,9))



import time

df_arr = []
for idx in tqdm(range(15)):
    recall_result = []
    precision_result = []
    prediction_result = []

    true_result = []
    correct_result = []
    spent_result = []

    x_test =  test_clusters[idx]
    test_tags = x_test['tag_id'].unique()
    total_songs = np.intersect1d(test_clusters[idx].song_id.unique(), Clusters[idx]._unique_songs)

    k = 50

    for _test_tag_id in tqdm(test_tags):
        song_ids = np.intersect1d(x_test.song_id[x_test.tag_id == _test_tag_id].values, total_songs)

        start_time = time.time()
        predicted = Clusters[idx].predict(idx2tag[_test_tag_id])
        spent_result.append(time.time() - start_time)

        correct_count = np.intersect1d(song_ids, predicted).shape[0]

        correct_result.append(correct_count)
        prediction_result.append(predicted)
        true_result.append(song_ids)

        if len(song_ids) == 0:
          recall_result.append(0)
          precision_result.append(0)
          continue
        
        recall_result.append(correct_count / len(song_ids))
        precision_result.append(correct_count / k)

    tag_names = [idx2tag[tag_id] for tag_id in test_tags]
    df_result = pd.DataFrame([tag_names, recall_result, precision_result, prediction_result, true_result, correct_result, spent_result]).T
    df_result.columns = ['tag', 'recall', 'precision', 'prediction', 'true', 'correct', 'spent']
    df_result['cluster'] = idx
    df_arr.append(df_result)

df_result_all = pd.concat(df_arr)

df_result_all.correct

df_result_all.recall = df_result_all.recall.astype(float)
df_result_all.spent = df_result_all.spent.astype(float)
df_result_all.precision = df_result_all.precision.astype(float)

df_result_all = df_result_all.reset_index(drop=True)
df_result_all.to_json(get_path('Result_Cluster_v2.json'))

df_result_all.groupby('cluster')['recall','precision'].mean().plot.bar(figsize=(16,9))

df_result_all.recall.mean(), df_result_all.precision.mean(), df_result_all.spent.mean()

good_tags = ['디즈니', '연애', '유재석', '고막남친', '포근한', '연말', '짝사랑', '운전', '봄바람',
       '잠들기전', '힘', '흥겨운', '신곡', '산책', '듣기좋은', '걸그룹', '파티', '비오는날', '추위',
       '비오는', '불토', '재즈', '이브', 'UK', '연애세포', '여행', '가을밤', '센치한', '록',
       '행복', '배경음악', '감성힙합', '추석', '추억', '지친', '집', '어쿠스틱', '감성곡', '드라이브',
       '고속도로', '신나는', '가을비', '트로트', '눈', '불금', '피아노', '발랄', '성인가요', '댄스',
       '주말', '패션', '우울한', '벚꽃', '날씨', '커피', '노동요', '수록곡', '힘내', '경쾌한',
       '정미애', '따뜻함', '노래', 'OfficialCharts', '헤어짐', '남자아이돌', '팝', '댄스곡',
       '정다경', '여유', '휴식', '새벽', '잔잔함', '2000년대', '나른', '우울', '12월',
       '뉴에이지', '우울할때', '명절', '2019년', '중독성', '엄마아리랑', '인디', '비', '자장가',
       '버스', '스타일', '축하', '까페', '퇴근', '2016', '상큼한', '노래방', '찾아오는DJ',
       'RNBSOUL', '상큼', '청량한', 'EDM', 'deep', '한국힙합', '아픔', '밝은', '혼자',
       '기분전환', '찬양', '아이돌', '부드러운', '눈오는날', '귀르가즘', '여름노래', '댄스댄스', 'bar',
       '크리스마스캐롤', '테라스', 'electronica', '연휴', '봄비', '가을', '북카페', '알앤비',
       '토닥토닥', '장마', '트렌디', '꿀잠', '잔잔', '하우스', '와인', '생일', '국외', '고백',
       '신남', '봄노래', '쓸쓸', '얼터너티브', '느낌있는', '성탄절', '브금', '불면증', '귀성길',
       '에이핑크', '디스코', '인기', '달달', '소울', '책읽을때', '슬픔', '운동', '사랑노래', '락',
       '잔잔한노래', '여름밤', '쌀쌀한', '시원한', '저녁', '낮잠', '일렉', '캐롤', '방콕',
       '메리크리스마스', '기분좋은', '따뜻하게', '빌로우', '아무로나미에', '장르구분없이', '월요병', '분위기',
       '청량', '페스티벌', '추천곡', '내적댄스', '국힙', '외로움', '클럽', '즐거운', '맑은', '스포츠',
       '열대야', '오르골', '감성발라드', '밤', '시작', '브릿팝', '산뜻한', 'EDMFloor', '빗소리',
       '쓸쓸함', '조용한', '낙엽', '겨울밤', '좋아요', '2019', '몽환', '블랙뮤직', '겨울노래',
       '겨울', '휴일', '태교', '팝송', '새벽감성', '바캉스', '그리움', '추천', 'Lounge',
       '플레이리스트', 'dance', '달달한', '신나는노래', '하늘', 'Instrumental', '그루브',
       '치유', '봄', '퓨전재즈', '회식', '일렉트로니카', '잔잔한', '힙합엘이', '힙한', '힘들때',
       '설렘', '취향저격', '독서', '썸', '차분', '목소리', '마음', '내한', '랩', '더위', '따듯한',
       '띵곡들', '베스트', '봄나들이', '인디팝', '연주곡', '셋리스트', '감성', '장윤정', '후회',
       '바람', '헬스', '지칠때', '흐린날', '감성적인', '유산슬', '임영웅', '집콕', '에너지',
       '몽환적인', 'Christmas', '해외일렉트로니카', '크리스마스', '바다', '케이팝', '기분업',
       '흥폭발', '겨울감성', '어린이집', '숨은명곡', 'OST', '2000', '고막여친', '춤', '휴가',
       'bgm', '트랩', '연인', '비오는날듣기좋은노래', '하루', '가족', '슬픈', 'Pop', '집중',
       '눈물', '아침', '카페뮤직', '두근두근', 'Rock', '차분한', '출근길', '이별', 'house',
       '기분', '스트레스', '봄날', '잠', 'ballad', '음색', '달콤', '인디음악', '기타',
       '다이어트', '띵곡', '떼창', '오후', '시험', '숙면', '회상', '대세', '캐럴', '취저',
       '달달한노래', '꿀성대', '희망', '팝송모음', '흥', '설레임', '힙합', '애창곡', '카페', '매장',
       '음색깡패', '크리스마스노래', '스웩', '오늘', '음악', '1990', '신나는음악', '유니크', '퇴근길',
       '신나', '잠잘때', '공부', '설날', '힘이_나는', '달콤한', '주제곡', '사랑', '발라드', '생각',
       '여름', '힐링', '여자아이돌', '방탄', '센치', '고백송', '커플', '첫사랑', '조깅', '지하철',
       '위로', '매장음악', '좋은노래', '가사', '데이트', '유산소', '트로피컬', '리드미컬', '솔로',
       '세련된', '비트', '스타일리시', '야경', '나들이', '편안한', '휘트니스', '영국', '일상']

df_result_all2 = df_result_all[df_result_all.tag.isin(good_tags)].reset_index(drop=True)

df_result_all.recall.mean(), df_result_all.precision.mean(), df_result_all.spent.mean()

df_result_all2.recall.mean(), df_result_all2.precision.mean(), df_result_all2.spent.mean()

df_result_all2.to_json(get_path('final_result_cluster_v1.json'))





















recall_sum = 0
for i, result in enumerate(df_arr):
  recall_sum += result[1].mean()
  print(f"{i}: {result[1].mean()}")

df_arr[1].sort_values(1, ascending=0).head(30)

recall_sum / 15

# Commented out IPython magic to ensure Python compatibility.
# %timeit predicted = Clusters[0].predict('성탄절')

# Commented out IPython magic to ensure Python compatibility.
# %timeit predicted = Clusters[1].predict('성탄절')

# Commented out IPython magic to ensure Python compatibility.
# %timeit predicted = Clusters[2].predict('성탄절')

# Commented out IPython magic to ensure Python compatibility.
# %timeit predicted = Clusters[3].predict('성탄절')

# Commented out IPython magic to ensure Python compatibility.
# %timeit predicted = Clusters[4].predict('성탄절')

# Commented out IPython magic to ensure Python compatibility.
# %timeit predicted = Clusters[5].predict('성탄절')

# Commented out IPython magic to ensure Python compatibility.
# %timeit predicted = Clusters[6].predict('성탄절')

# Commented out IPython magic to ensure Python compatibility.
# %timeit predicted = Clusters[7].predict('성탄절')

# Commented out IPython magic to ensure Python compatibility.
# %timeit predicted = Clusters[8].predict('성탄절')

# Commented out IPython magic to ensure Python compatibility.
# %timeit predicted = Clusters[9].predict('성탄절')

# Commented out IPython magic to ensure Python compatibility.
# %timeit predicted = Clusters[10].predict('성탄절')

# Commented out IPython magic to ensure Python compatibility.
# %timeit predicted = Clusters[11].predict('성탄절')

# Commented out IPython magic to ensure Python compatibility.
# %timeit predicted = Clusters[12].predict('성탄절')

# Commented out IPython magic to ensure Python compatibility.
# %timeit predicted = Clusters[13].predict('성탄절')

# Commented out IPython magic to ensure Python compatibility.
# %timeit predicted = Clusters[14].predict('성탄절')

204+118+111+43.4+192+253+431+261+127+901+338+486+37+281+173

(3956.4) / 15



train_tags = train_clusters[0].tag_id.unique()

df_result['tag_id'] = df_result[0].map(lambda x: tag2idx.get(x))

df_result.sort_values(2, ascending=False).head(10)

# Commented out IPython magic to ensure Python compatibility.
# %time predicted = Clusters[0].predict('불금')

df_meta[df_meta.id.isin(predicted)]









